Une des caractéristiques du langage R sur laquel sa réputation s'est bâtie est la variété des outils statistiques qu'il place à la disposition de son utilisateur. Sans même avoir à importer une quelconque librairie à partir de CRAN, plusieurs distributions statistiques sont disponibles. La \autoref{tab:distStatsR} fait la revue des ces distributions et de leur identifiant R correspondant. \cite{distStatsR} \\

\begin{table}[h]
	\centering
	\begin{tabular}{cc}
		\textbf{Distribution} & \textbf{identifiant R} \\
		\hline
		Bêta & beta \\
		Binomiale & binom \\
		Binomiale négative & nbinom \\
		Chi Deux & chisq \\
		Exponentielle & exp \\
		Fisher & f \\
		Gamma & gamma \\
		Géometrique & geom \\
		Hypergéometrique & hyper \\
		Normale & norm \\
		Poisson & pois \\
		Student & t \\
		Uniforme & unif \\
		Weibull & weibull	
	\end{tabular}
	\caption{Liste des distributions statistiques disponibles en R}
\end{table}
\label{tab:distStatsR}

D'autres distributions deviendront aussi disponibles via des paquetages dédiés à cette fin. Le paquetage \texttt{actuar} \cite{Rpackage:actuar} donne accès à plusieurs distributions supplémentaires communément utilisées en actuariat. La distribution \emph{Pareto} en est un bon exemple. \\

Un aspect particulièrement intéressant de ces fonctions (qu'elles soient disponibles par défaut en R ou via l'importation d'un paquetage) est la constance dans leur implémentation. Pour chacune des distributions, nous retrouverons minimalement les quatre fonctions qui suivent: \\

\begin{description}[style=multiline,leftmargin=2cm]
	\item[$d\langle ID_R \rangle$] Calcule la valeur de la fonction de densité de la distribution ayant l'identifiant R $\langle ID_R \rangle$.
	\item[$p\langle ID_R \rangle$] Calcule la valeur de la fonction de répartition de la distribution ayant l'identifiant R $\langle ID_R \rangle$.
	\item[$q\langle ID_R \rangle$] Renvoie le quantile associé à la valeur fournie en argument selon  la fonction de répartition de la distribution ayant l'identifiant R $\langle ID_R \rangle$.
	\item[$r\langle ID_R \rangle$] Permet de générer des valeurs aléatoires suivants la distribution ayant l'identifiant R $\langle ID_R \rangle$.
\end{description}

De plus, les arguments de ces fonctions se présenteront toujours sous le même format. Nous devrons soit fournir la valeur à laquelle nous voulons évaluer la fonction ou encore un nombre d'observations à générer dans le cas des fonctions préfixées par "r" et les paramètres de la loi utilisée. À des fins d'optimisation des performances, le logarithme de ces fonctions sera souvent nécessaire et c'est ce qui explique la présence de l'argument \texttt{log}. \footnote{Plusieurs propriétés statistiques découlent du logarithme des fonctions de densité et de répartition tel que la fonction génératrice de moments pour ne nommer que cette denière.} Finalement, nous serons parfois intéressés par la fonction de survie d'une distribution donnée correspondant au complément de la fonction de répartition. En attribuant la valeur \textit{FALSE} à l'argument \texttt{lower.tail}, les fonctions préfixées par "p" renverront ainsi la valeur de la fonction de survie. Un exemple d'utilisation de ces fonctions est présenté par le \autoref{src:distStats}. \\

\begin{lstlisting}[caption = Fonctions relatives à la distribution Normale,label=src:distStats]
> set.seed(2017)
> mean <- 6
> sd <- 2
> x <- 0:12
> dnorm(x,mean,sd)
 [1] 0.002215924 0.008764150 0.026995483 0.064758798
 [5] 0.120985362 0.176032663 0.199471140 0.176032663
 [9] 0.120985362 0.064758798 0.026995483 0.008764150
[13] 0.002215924
> pnorm(x,mean,sd)
 [1] 0.001349898 0.006209665 0.022750132 0.066807201
 [5] 0.158655254 0.308537539 0.500000000 0.691462461
 [9] 0.841344746 0.933192799 0.977249868 0.993790335
[13] 0.998650102
> r <- seq(0,1,0.1)
> qnorm(r,mean,sd)
 [1]     -Inf 3.436897 4.316758 4.951199 5.493306
 [6] 6.000000 6.506694 7.048801 7.683242 8.563103
[11]      Inf
> rnorm(10,mean,sd)
 [1] 8.868403 5.845416 7.478274 2.482791 5.860350
 [6] 6.903811 2.083267 5.996951 5.469328 9.126445
\end{lstlisting}

Ceux qui sont familiers avec les distributions statistiques auront remarqué qu'à l'aide des fonctions décrites ci-dessus nous aurons donc deux manières de générer des nombres aléatoires. La première qui est aussi la plus évidente sera d'utiliser les fonctions préfixées avec "r". La seconde utilisera le théroême de la réciproque qui consiste à générer des valeurs aléatoires suivant une loi uniforme de paramètre $a := 0$ et $b := 1$ pour ensuite trouver les quantiles de la loi pour laquelle nous voulons générer des nombres aléatoires grâce aux fonctions préfixées par "q". Ces deux techniques sont mises à profit dans le \autoref{src:aleaNorm}. \\

\begin{lstlisting}[caption = Génération de nombres aléatoires,label=src:aleaNorm]
> y1 <- rnorm(1000,mean,sd)
> summary(y1)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
 0.07041  4.70800  6.02800  6.06200  7.35500 12.59000 
> sd(y1)
[1] 1.96455
> r <- runif(1000)
> y2 <- qnorm(r,mean,sd)
> summary(y2)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-0.1347  4.7670  6.0830  6.0910  7.5070 12.2400 
> sd(y2)
[1] 1.966951
\end{lstlisting}

\begin{moreInfo}{Théorème de la réciproque}
	Ce sont les 4 propriétés des fonctions de répartition qui rendent possible l'application du théorème de la réciproque. Ces propriétés sont définies comme suit (où $F$ désigne la fonction de répartition d'une variable aléatoire $X$ quelconque):
	\begin{enumerate}
		\item $F_X$ est croissante
		\item Elle est partout continue à droite
		\item $lim_{x \rightarrow -\infty} F_X(x) = 0$
		\item $lim_{x \rightarrow \infty} F_X(x) = 1$
	\end{enumerate}
	Étant donné que ces propriétés seront toujours respectées pour toute fonction de répartition, nous pourrons appliquer cette méthode, peu importe la distribution qu'elle soit clairement définie ou non! \\
	\url{https://fr.wikipedia.org/wiki/Fonction_de_r%C3%A9partition#Th.C3.A9or.C3.A8me_de_la_r.C3.A9ciproque}
\end{moreInfo}

En présence de données empiriques, la première étape d'une analyse statistique sera de dresser le portrait statistique de ces données. Nous avons déjà parlé de la fonction \texttt{summary} à la \autoref{subsec:extraction}. Nous rajouterons ici les fonctions \texttt{mean} et \texttt{sd} retournant respectivement la moyenne et l'écart-type d'un jeu de données empiriques (\autoref{src:aleaNorm}). \\

Afin de valider l'ajustement d'une distribution sur les données empiriques, nous serons souvent contraints à identifier les fonctions de densité et de répartition sous-jacentes. Il existe plusieurs façons de faire. Celle qui nous semble toutefois la plus pertinente et polyvalente exploite le comportement de la fonction \texttt{ecdf}. Cette dernière permet de construire une fonction de répartition empirique à partir des observations fournies en argument. Nous pouvons ensuite construire une fonction de densité empirique en évaluant cette fonction de répartition à deux points autour de la valeur désirée et en divisant ensuite le résultat par la largeur de l'intervalle évalué. Les instructions permettant de construire ces fonctions sont fournies par \autoref{src:fctEmp}. \\

\begin{lstlisting}[caption = Fonctions de densité et de répartition empiriques,label=src:fctEmp]
empCDF <- ecdf(compData$weight)
empPDF <- function(x,delta=0.01)
{
  (empCDF(x+delta/2)-empCDF(x-delta/2))/delta
}
\end{lstlisting}

\vspace{\baselineskip}
En plus de dresser le portrait statistique des données, on peut aussi vouloir faire des tests statistiques à partir de celles-ci. Parmi les tests disponibles, nous retrouvons notamment :
\begin{itemize}
	\item Test de normalité (Test de Shapiro-Wilk)
	\item Test de comparaison de deux variances (Test F)
	\item Test de Student
	\item Test du Khi carré
	\item Test de Wilcoxon
	\item ANOVA (Analyse de variance)
	\item Test de corrélation
\end{itemize}
Il n'est toutefois pas indispensable de connaître l'utilité de tous ces tests, les situations dans lesquelles ils devront être utilisés ni la mécanique mathématique sous-entendue puisque la plupart des méthodes statistiques incluront déjà les appels nécessaires de ceux-ci. Ce sera le cas de la fonction \texttt{lm} comme nous le verrons plus loin. \cite{testStatsR} \\

Dans le cadre de notre étude de cas, nous avons performé les tests du Khi carré et de corrélation afin de s'assurer que les variables explicatives du poids et de la distance soient indépendantes et sans corrélation. Dans le cas où ce genre de phénomène serait apparu entre nos variables, nous aurions été obligés d'utiliser des modèles de régression plus complexes tels que les modèles linéaires généralisés. \\

Lorsque nous effectuons un test statistique, nous cherchons toujours à répondre à une question binaire représentée sous la forme de deux hypothèses $H_0$ et $H_1$ complémentaires. Une valeur nommée la \texttt{p-value} sera ensuite calculée en acceptant l'hypothèse $H_0$ comme vraie. Cette valeur correspondra à la probabilité d'observer un résultat équivalent ou supérieur du test que nous venons d'exécuter en considérant l'hypothèse nulle comme vraie. En d'autres mots, cette valeur nous indiquera la probabilité de se tromper en rejetant l'hypothèse nulle en considérant l'hypothèse nulle comme vraie initialement. Ainsi, à partir du moment où la \texttt{p-value} sera inférieure au seuil de crédibilité que l'on s'était fixé (habituellement 5\%), nous considérerons l'hypothèse nulle comme fausse. \\ 

Dans le cas du test du Khi carré, l'hypothèse nulle suppose que les deux distributions sont indépendantes. Le test de corrélation suppose tant qu'à lui que la valeur théorique de corrélation est équivalente à 0. Comme nous pouvons le voir avec le \autoref{src:testStat}, nous ne pouvons pas rejeter ces deux hypothèses. \\

\begin{lstlisting}[caption = Tests d'indépendance et de corrélation entre distributions,label=src:testStat]
> weightsBinded <- cut(compData$weight,25)
> distancesBinded <- cut(compData$distance,25)
> contingencyTable <- table(weightsBinded,distancesBinded)
> rownames(contingencyTable) <- NULL
> colnames(contingencyTable) <- NULL
> chisq.test(contingencyTable)

	Pearson's Chi-squared test

data:  contingencyTable
X-squared = NaN, df = 576, p-value = NA

Warning message:
In chisq.test(contingencyTable) :
  Chi-squared approximation may be incorrect
> contingencyTable <- rbind(contingencyTable[1:6,],colSums(contingencyTable[7:25,]))
> contingencyTable <- cbind(contingencyTable[,1:12],rowSums(contingencyTable[,13:25]))
> (independencyTest <- chisq.test(contingencyTable))

	Pearson's Chi-squared test

data:  contingencyTable
X-squared = 52.312, df = 72, p-value = 0.961

> cor.test(compData$weight,compData$distance,method = "pearson")

	Pearson's product-moment correlation

data:  compData$weight and compData$distance
t = 0.89049, df = 99998, p-value = 0.3732
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.003382045  0.009013785
sample estimates:
        cor 
0.002815978 
\end{lstlisting}

\vspace{\baselineskip}
Il est pertinent d'évoquer que le test du Khi carré possède des limitations importantes dans le cas de distributions devenant un peu trop clairsemées dans les extrêmes. C'est pour cette même raison que nous combinons les dernières lignes et colonnes de la table de contingence. \cite{Rfunction:chisqWarning} La \texttt{p-value} d'environ 96\% nous indique que l'hypothèse nulle est juste. Dans le cas du test de corrélation, nous voyons que la valeur 0 est comprise dans notre intervalle de confiance autour de la valeur de corrélation empirique déterminée, ce qui nous permet d'affirmer qu'aucune corrélation n'existe entre ces deux variables. La \texttt{p-value} de 37\% aurait été suffisante pour arriver à la même conclusion. \\

Pour terminer cette section, jetons un coup d'oeil à la régression linéaire qui fut accomplie dans le but de modéliser la distribution ayant mené à générer les données (\autoref{src:linReg}).

\begin{lstlisting}[caption = Régression linéaire sur données empiriques,label=src:linReg]
> profitMargin <- 1.12
> avgTaxRate <- sum(table(airportsCanada$province)*as.numeric(paste(taxRates$taxRate)))/
+   length(airportsCanada$province)
> compModel <- lm(price/(profitMargin*avgTaxRate) ~ distance + weight, compData)
> summary(compModel)

Call:
lm(formula = price/(profitMargin * avgTaxRate) ~ distance + weight, 
    data = compData)

Residuals:
     Min       1Q   Median       3Q      Max 
-30.0086  -4.6571   0.0157   4.6462  30.4167 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 3.228e+01  7.510e-02  429.83   <2e-16 ***
distance    2.819e-02  9.202e-05  306.28   <2e-16 ***
weight      7.254e-01  9.607e-03   75.51   <2e-16 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.932 on 99997 degrees of freedom
Multiple R-squared:  0.4991,	Adjusted R-squared:  0.4991 
F-statistic: 4.982e+04 on 2 and 99997 DF,  p-value: < 2.2e-16
\end{lstlisting}

\vspace{\baselineskip}
L'appel de la fonction \texttt{lm} \cite{Rfunction:lm} est assez élémentaire. Il suffit de fournir une formule de régression contenant les variables explicatives avec lesquelles nous tenons à faire la régression et nous spécifions le nom de la table contenant ces variables. Nous remarquons ici la technique du retour multiple abordée à la \autoref{sec:fctUtil}. Nous voyons aussi que pour chaque coefficient un test de Student a été effectué pour déterminer à quel point les estimations sont significativement différentes de 0. D'autre part, le test de Fisher permet de savoir s'il existe réellement une relation entre les variables explicatives choisies et la variable réponse analysée. \cite{outputLM} \\

Lorsque l'on compare les valeurs réellement utilisées dans le \autoref{src:Benchmark} et les coefficients estimés, nous voyons que ces derniers sont très proches les uns des autres. La \autoref{tab:linRegFit} fait la revue de ces valeurs. \\

\begin{table}[h]
	\centering
	\begin{tabular}{ccc}
		\textbf{Variable} & \textbf{Valleur réelle} & \textbf{Valeur estimée} \\
		\hline
		distance & 0.0275 & 0.02819 \\
		poids & 0.7 & 0.7254		
	\end{tabular}
	\caption{Comparaison entre les coefficients réels et estimés par régression linéaire}
	\label{tab:linRegFit}
\end{table}

\begin{moreInfo}{Lire des tables directement sur le web}
	Pour récupérer les niveaux de taxe pour chacune des provinces canadiennes, nous avons pris l'initiative de passer directement via le web. Cette méthode possède l'avantage de se mettre à jour directement avec l'information la plus récente \footnote{Nous serons toutefois dépendants de la structure de la page et tout changement majeur pourra compromettre la suite du programme. Des procédés de validation devront être mis en place si le script est mis en production.} chaque fois que le script sera exécuté. Afin de parvenir à ce résultat, les paquetages \texttt{XML}, \texttt{RCurl} et \texttt{rlist} fournissent des fonctions permettant d'interpréter la structure \texttt{HTML} d'une page web spécifiée par le passage du chemin \texttt{url} en argument de la fonction \texttt{readHTMLTable}. Cette dernière ira lire le code source de la page web en question pour y détecter les occurrences des balises $\langle table \rangle$, $\langle tr \rangle$, $\langle th \rangle$ et $\langle td \rangle$.  $$\langle table \rangle \langle td \rangle \dots \langle /td \rangle \langle /table \rangle$$
	\url{http://web.mit.edu/~r/current/arch/i386_linux26/lib/R/library/XML/html/readHTMLTable.html}
\end{moreInfo}